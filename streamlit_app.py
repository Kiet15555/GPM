# T·ªîNG H·ª¢P T·ª™ STEP 1 ƒê·∫æN 20 (PHI√äN B·∫¢N ƒê·∫¶Y ƒê·ª¶ - ƒê√É FIX L·ªñI WEBGL & C·ªê ƒê·ªäNH RANDOM SEED)
import streamlit as st
import pandas as pd
import numpy as np
import requests
from datetime import datetime
import warnings
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
import os
import plotly.graph_objects as go
import plotly.express as px
import plotly.io as pio
from plotly.subplots import make_subplots 
from scipy.optimize import minimize
import quantstats as qs
from io import BytesIO
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
import base64  # S·ª¨A 3: Import ƒë·ªÉ m√£ h√≥a ·∫£nh
import streamlit.components.v1 as components  # S·ª¨A 3: Import ƒë·ªÉ d√πng HTML

# --- C·∫•u h√¨nh Trang & C√°c thi·∫øt l·∫≠p ban ƒë·∫ßu ---
warnings.filterwarnings('ignore')
st.set_page_config(layout="wide", page_title="T·ªëi ∆∞u Danh m·ª•c VN30 Pro")
# S·ª¨A 1 & 2: X√≥a icon v√† (B·∫£n Full)
# [S·ª¨A 4] CƒÇN GI·ªÆA TI√äU ƒê·ªÄ
st.markdown("<h1 style='text-align: center;'>·ª®ng d·ª•ng Ph√¢n t√≠ch & T·ªëi ∆∞u h√≥a Danh m·ª•c VN30</h1>", unsafe_allow_html=True)


# --- [S·ª¨A 3] THAY BANNER Tƒ®NH B·∫∞NG SLIDESHOW ---

# H√†m ƒë·ªÉ ƒë·ªçc v√† m√£ h√≥a ·∫£nh sang Base64
def get_image_base64(path):
    try:
        with open(path, "rb") as img_file:
            return base64.b64encode(img_file.read()).decode('utf-8')
    except FileNotFoundError:
        return None

# [S·ª¨A 6] Th√™m banner6.jpg v√†o danh s√°ch
image_paths = ["banner1.jpg", "banner2.jpg", "banner3.jpg", "banner4.jpg", "banner5.jpg", "banner6.jpg"]
base64_images = []
for path in image_paths:
    b64_img = get_image_base64(path)
    if b64_img:
        base64_images.append(b64_img)

if base64_images:
    # T·∫°o chu·ªói HTML cho c√°c ·∫£nh
    html_images = ""
    for b64_img in base64_images:
        # Gi·∫£ ƒë·ªãnh t·∫•t c·∫£ l√† jpeg, c√≥ th·ªÉ c·∫ßn ƒë·ªïi n·∫øu l√† png
        html_images += f'<img class="mySlides fade" src="data:image/jpeg;base64,{b64_img}" style="width:100%">'
    
    # T·∫°o m√£ HTML/CSS/JS cho slideshow
    html_code = f"""
    <style>
    .slideshow-container {{
      width: 100%;
      position: relative;
      margin: auto;
      /* [S·ª¨A 6] X√≥a max-height ƒë·ªÉ ·∫£nh kh√¥ng b·ªã c·∫Øt */
      /* max-height: 450px; */ 
      overflow: hidden;
      border-radius: 8px; /* Bo g√≥c */
    }}
    .mySlides {{
      display: none; /* ·∫®n t·∫•t c·∫£ ·∫£nh ban ƒë·∫ßu */
      width: 100%;
      /* [S·ª¨A 6] ƒê·∫∑t chi·ªÅu cao t·ª± ƒë·ªông ƒë·ªÉ ·∫£nh kh√¥ng b·ªã c·∫Øt */
      height: auto; 
      object-fit: contain; /* [S·ª¨A 6] ƒê·ªïi t·ª´ cover sang contain ƒë·ªÉ ·∫£nh hi·ªÉn th·ªã to√†n b·ªô */
      vertical-align: middle;
    }}
    /* Hi·ªáu ·ª©ng m·ªù d·∫ßn */
    .fade {{
      animation-name: fade;
      animation-duration: 3s; /* Gi·ªØ nguy√™n 3s ƒë·ªÉ m·ªÅm m·∫°i */
    }}
    @keyframes fade {{
      from {{opacity: .4}}
      to {{opacity: 1}}
    }}
    </style>

    <div class="slideshow-container">
      {html_images}
    </div>

    <script>
    let slideIndex = 0;
    showSlides(); // B·∫Øt ƒë·∫ßu slideshow

    function showSlides() {{
      let i;
      let slides = document.getElementsByClassName("mySlides");
      if (slides.length === 0) return; // Kh√¥ng c√≥ ·∫£nh th√¨ d·ª´ng
      
      // ·∫®n t·∫•t c·∫£ ·∫£nh
      for (i = 0; i < slides.length; i++) {{
        slides[i].style.display = "none";
      }}
      
      slideIndex++;
      if (slideIndex > slides.length) {{slideIndex = 1}} // Quay l·∫°i ·∫£nh ƒë·∫ßu ti√™n
      
      slides[slideIndex-1].style.display = "block"; // Hi·ªÉn th·ªã ·∫£nh hi·ªán t·∫°i
      
      setTimeout(showSlides, 5000); // Gi·ªØ nguy√™n 5 gi√¢y chuy·ªÉn ·∫£nh
    }}
    </script>
    """
    # [S·ª¨A 6] ƒê·ªÉ chi·ªÅu cao c·ªßa component linh ho·∫°t, c√≥ th·ªÉ b·ªè height ho·∫∑c d√πng 1 gi√° tr·ªã l·ªõn h∆°n n·∫øu c·∫ßn.
    # Ho·∫∑c ƒë·ªÉ tr·ªëng n·∫øu mu·ªën n√≥ t·ª± ƒë·ªông co gi√£n ho√†n to√†n theo n·ªôi dung
    components.html(html_code, height=500) # TƒÉng nh·∫π chi·ªÅu cao m·∫∑c ƒë·ªãnh cho HTML component
else:
    st.warning("""
    Kh√¥ng t√¨m th·∫•y file banner! Vui l√≤ng:
    1. ƒê·ªïi t√™n c√°c file ·∫£nh th√†nh: `banner1.jpg`, `banner2.jpg`, `banner3.jpg`, `banner4.jpg`, `banner5.jpg`, `banner6.jpg`
    2. ƒê·∫∑t c√°c file n√†y v√†o C√ôNG TH∆Ø M·ª§C v·ªõi t·ªáp `streamlit_app.py`
    """)
# --- K·∫æT TH√öC S·ª¨A BANNER ---


# Set theme m·∫∑c ƒë·ªãnh cho Plotly
pio.templates.default = "plotly_dark"
# ... (Ph·∫ßn c√≤n l·∫°i c·ªßa code kh√¥ng thay ƒë·ªïi) ...
# === C√ÅC H√ÄM ƒê·ªäNH NGHƒ®A (G·ªòP T·ª™ C√ÅC FILE) ===

# --- T·ª´ Step2.py (√î 2) ---
def get_price_history_api(symbol: str, start_date: datetime, end_date: datetime):
    all_data = []
    page = 1
    total_pages = 1
    while page <= total_pages:
        url = "https://cafef.vn/du-lieu/Ajax/PageNew/DataHistory/PriceHistory.ashx"
        params = {"Symbol": symbol, "StartDate": start_date.strftime("%Y-%m-%d"),
                  "EndDate": end_date.strftime("%Y-%m-%d"), "PageIndex": page}
        try:
            response = requests.get(url, params=params, timeout=10)
            response.raise_for_status()
            data = response.json()
            if not data.get("Success", False): break
            records = data["Data"]["Data"]
            if not records: break
            if page == 1:
                total_count = data["Data"]["TotalCount"]
                total_pages = -(-total_count // len(records))
            all_data.extend(records)
            page += 1
        except Exception as e:
            print(f"L·ªói khi g·ªçi API CafeF cho {symbol}: {e}")
            return None
    if not all_data: return None
    df = pd.DataFrame(all_data)
    df['Ticker'] = symbol.upper()
    numeric_columns = ['GiaDieuChinh', 'GiaDongCua', 'KhoiLuongKhopLenh', 
                      'GiaTriKhopLenh', 'GiaMoCua', 'GiaCaoNhat', 'GiaThapNhat']
    for col in numeric_columns: df[col] = pd.to_numeric(df[col], errors='coerce')
    df = df.sort_values('Ngay', ascending=True).reset_index(drop=True)
    df['GiaDongCua'].replace(0, np.nan, inplace=True); df['GiaDongCua'] = df['GiaDongCua'].ffill().bfill()
    df.loc[df['GiaDongCua'] == 0, 'GiaDieuChinh'] = df['GiaDieuChinh']
    df.loc[df['GiaDongCua'] == 0, 'GiaDongCua'] = 1
    df['adjustment_ratio'] = df['GiaDieuChinh'] / df['GiaDongCua']
    df['open_adj'] = df['GiaMoCua'] * df['adjustment_ratio']
    df['high_adj'] = df['GiaCaoNhat'] * df['adjustment_ratio']
    df['low_adj'] = df['GiaThapNhat'] * df['adjustment_ratio']
    df = df.rename(columns={'Ngay': 'time', 'open_adj': 'open', 'high_adj': 'high',
                            'low_adj': 'low', 'GiaDieuChinh': 'close', 
                            'KhoiLuongKhopLenh': 'volume', 'Ticker': 'ticker'})
    df['time'] = pd.to_datetime(df['time'], format="%d/%m/%Y")
    return df[['time', 'open', 'high', 'low', 'close', 'volume', 'ticker']].sort_values('time').reset_index(drop=True)

# --- T·ª´ Step3.py (√î 3) ---
def get_stock_data(tickers: list, start_date: str, end_date: str) -> pd.DataFrame:
    st.info(f"B·∫Øt ƒë·∫ßu l·∫•y d·ªØ li·ªáu cho {len(tickers)} m√£ (ƒêa lu·ªìng - Th√¢n thi·ªán)...")
    all_data = []
    start_dt = datetime.strptime(start_date, '%Y-%m-%d')
    end_dt = datetime.strptime(end_date, '%Y-%m-%d')
    
    def fetch_one_ticker_isolated(ticker):
        time.sleep(0.2) 
        df_ticker = get_price_history_api(ticker, start_dt, end_dt) 
        if df_ticker is not None and not df_ticker.empty:
            return ticker, df_ticker
        else:
            return ticker, None

    progress_text = st.empty()
    progress_bar = st.progress(0)
    total_tickers = len(tickers)
    completed_count = 0

    with ThreadPoolExecutor(max_workers=5) as executor:
        futures = {executor.submit(fetch_one_ticker_isolated, ticker): ticker for ticker in tickers}
        for future in as_completed(futures):
            ticker, df_ticker = future.result()
            completed_count += 1
            progress_bar.progress(completed_count / total_tickers)
            progress_text.text(f"ƒêang x·ª≠ l√Ω m√£ {completed_count}/{total_tickers}: {ticker}...")
            
            if df_ticker is not None:
                all_data.append(df_ticker)
            else:
                st.warning(f"(!) Kh√¥ng t√¨m th·∫•y d·ªØ li·ªáu cho m√£: {ticker}")
    
    progress_text.empty()
    progress_bar.empty()

    if not all_data:
        st.error("(!) Kh√¥ng l·∫•y ƒë∆∞·ª£c b·∫•t k·ª≥ d·ªØ li·ªáu n√†o.")
        return pd.DataFrame()

    final_df = pd.concat(all_data, ignore_index=True)
    final_df = final_df.sort_values(by=['ticker', 'time']).reset_index(drop=True)
    st.success("‚úÖ L·∫•y d·ªØ li·ªáu (ƒêa lu·ªìng) th√†nh c√¥ng!")
    return final_df

# --- H√†m t·ªïng h·ª£p t·ª´ Step5.py (√î 4) ---
def load_data(tickers_list, start_time_str, end_time_str, force_refresh):
    CACHE_FILE = 'vn30_data_cache.parquet'
    
    if os.path.exists(CACHE_FILE) and not force_refresh:
        st.info(f"--- ƒêang t·∫£i d·ªØ li·ªáu t·ª´ Cache ({CACHE_FILE}) ---")
        try:
            raw_data = pd.read_parquet(CACHE_FILE)
            st.success("‚úÖ T·∫£i t·ª´ Cache th√†nh c√¥ng!")
            return raw_data
        except Exception as e:
            st.warning(f"L·ªói ƒë·ªçc file cache: {e}. S·∫Ω t·∫£i l·∫°i t·ª´ API.")

    if force_refresh:
        st.warning("--- B·∫Øt bu·ªôc l√†m m·ªõi (Force Refresh) ---")
    else:
        st.info("--- L·∫ßn ch·∫°y ƒë·∫ßu ti√™n (Cache not found) ---")

    raw_data = get_stock_data(tickers_list, start_time_str, end_time_str)

    if not raw_data.empty:
        try:
            st.info(f"--- ƒêang l∆∞u v√†o Cache ({CACHE_FILE}) ---")
            raw_data.to_parquet(CACHE_FILE)
            st.success("‚úÖ D·ªØ li·ªáu ƒë√£ t·∫£i v√† l∆∞u v√†o Cache.")
        except Exception as e:
            st.error(f"L·ªñI khi l∆∞u Cache: {e}. Vui l√≤ng c√†i ƒë·∫∑t 'pip install pyarrow'")
    
    return raw_data

# --- T·ª´ Step7.py (√î 6) ---
def calculate_stats(returns_df: pd.DataFrame, he_so_scale: int) -> tuple:
    expected_returns = returns_df.mean() * he_so_scale
    cov_matrix = returns_df.cov() * he_so_scale
    return expected_returns, cov_matrix

# --- T·ª´ Step11.py (√î 10) ---
def get_portfolio_stats(weights: np.array, 
                        expected_returns: pd.Series, 
                        cov_matrix: pd.DataFrame, 
                        risk_free_rate: float) -> tuple:
    port_return = np.sum(weights * expected_returns)
    port_risk = np.sqrt(np.dot(weights.T, np.dot(cov_matrix, weights)))
    port_sharpe = (port_return - risk_free_rate) / (port_risk + 1e-9) 
    return (port_return, port_risk, port_sharpe)

def minimize_negative_sharpe(weights: np.array, 
                             expected_returns: pd.Series, 
                             cov_matrix: pd.DataFrame,
                             risk_free_rate: float) -> float:
    return -get_portfolio_stats(weights, expected_returns, cov_matrix, risk_free_rate)[2]

def minimize_portfolio_risk(weights: np.array, 
                            expected_returns: pd.Series, 
                            cov_matrix: pd.DataFrame,
                            risk_free_rate: float) -> float:
    return get_portfolio_stats(weights, expected_returns, cov_matrix, risk_free_rate)[1]

# --- [M·ªöI] H√†m t√≠nh ƒë∆∞·ªùng bi√™n hi·ªáu qu·∫£ l√Ω thuy·∫øt ---
def calculate_theoretical_efficient_frontier(mean_returns, cov_matrix, risk_free_rate, num_points=100):
    num_assets = len(mean_returns)
    args = (mean_returns, cov_matrix, risk_free_rate)
    bounds = tuple((0.0, 1.0) for _ in range(num_assets))
    init_guess = num_assets * [1. / num_assets,]

    constraints_min_vol = ({'type': 'eq', 'fun': lambda x: np.sum(x) - 1})
    opt_min_vol = minimize(minimize_portfolio_risk, init_guess, args=args,
                           method='SLSQP', bounds=bounds, constraints=constraints_min_vol)
    
    min_ret_global = np.sum(mean_returns * opt_min_vol.x)
    max_ret_global = mean_returns.max() 

    target_returns = np.linspace(min_ret_global, max_ret_global, num_points)
    efficient_risks = []
    real_returns = []

    for target in target_returns:
        constraints = (
            {'type': 'eq', 'fun': lambda x: np.sum(x) - 1}, 
            {'type': 'eq', 'fun': lambda x: np.sum(mean_returns * x) - target}
        )
        opt = minimize(minimize_portfolio_risk, init_guess, args=args,
                       method='SLSQP', bounds=bounds, constraints=constraints)
        if opt.success:
            efficient_risks.append(opt.fun)
            real_returns.append(target)
            
    return pd.DataFrame({'Risk': efficient_risks, 'Return': real_returns})

# --- T·ª´ Step10.py (√î 8) ---
def run_monte_carlo_sim(n_sims: int, 
                        expected_returns: pd.Series, 
                        cov_matrix: pd.DataFrame, 
                        risk_free_rate: float) -> pd.DataFrame:
    # [FIX] C·ªë ƒë·ªãnh Seed ƒë·ªÉ k·∫øt qu·∫£ kh√¥ng b·ªã nh·∫£y lung tung m·ªói l·∫ßn ch·∫°y
    np.random.seed(42) 
    
    num_assets = len(expected_returns)
    results = np.zeros((3, n_sims))
    weights_record = []
    
    for i in range(n_sims):
        weights = np.random.random(num_assets)
        weights /= np.sum(weights)
        weights_record.append(weights)
        
        port_return, port_risk, port_sharpe = get_portfolio_stats(
            weights, expected_returns, cov_matrix, risk_free_rate
        )
        
        results[0, i] = port_return
        results[1, i] = port_risk
        results[2, i] = port_sharpe

    results_df = pd.DataFrame(results.T, columns=['Return', 'Risk', 'Sharpe'])
    weights_df = pd.DataFrame(weights_record, columns=expected_returns.index)
    sim_data_df = pd.concat([results_df, weights_df], axis=1)
    
    return sim_data_df

# --- T·ª´ Step16.py (√î 13) ---
def run_simple_backtest(daily_returns_df: pd.DataFrame, 
                        portfolio_weights: np.array) -> tuple:
    port_returns_daily = daily_returns_df.dot(portfolio_weights)
    port_returns_daily = pd.Series(port_returns_daily, index=daily_returns_df.index)
    cumulative_returns = (1 + port_returns_daily).cumprod()
    return port_returns_daily, cumulative_returns

# --- [M·ªöI] H√†m ph√¢n t√≠ch Rebalancing ---
def analyze_rebalancing(returns_df, target_weights, rebalance_freq='Q', transaction_cost=0.001):
    """
    Ph√¢n t√≠ch chi·∫øn l∆∞·ª£c rebalancing
    rebalance_freq: 'M' (Monthly), 'Q' (Quarterly), 'Y' (Yearly)
    transaction_cost: Chi ph√≠ giao d·ªãch (%)
    """
    # X√°c ƒë·ªãnh t·∫ßn su·∫•t rebalance (s·ªë ng√†y)
    if rebalance_freq == 'M':
        rebalance_days = 21  # ~1 th√°ng
    elif rebalance_freq == 'Q':
        rebalance_days = 63  # ~3 th√°ng
    else:  # 'Y'
        rebalance_days = 252  # ~1 nƒÉm
    
    dates = returns_df.index
    n_periods = len(dates)
    
    # Kh·ªüi t·∫°o gi√° tr·ªã danh m·ª•c
    rebal_value = 1.0
    bh_value = 1.0
    
    rebal_values = [rebal_value]
    bh_values = [bh_value]
    rebalance_dates = []
    total_costs = 0
    
    # Gi√° tr·ªã tuy·ªát ƒë·ªëi c·ªßa t·ª´ng asset (kh√¥ng ph·∫£i %)
    rebal_assets = target_weights.copy() * rebal_value
    bh_assets = target_weights.copy() * bh_value
    
    days_since_rebalance = 0
    
    for i in range(1, n_periods):
        daily_returns = returns_df.iloc[i].values
        days_since_rebalance += 1
        
        # === REBALANCED PORTFOLIO ===
        # C·∫≠p nh·∫≠t gi√° tr·ªã t·ª´ng asset theo returns
        rebal_assets = rebal_assets * (1 + daily_returns)
        rebal_value = rebal_assets.sum()
        
        # Ki·ªÉm tra rebalance
        if days_since_rebalance >= rebalance_days:
            # T√≠nh t·ª∑ tr·ªçng hi·ªán t·∫°i
            current_weights = rebal_assets / rebal_value
            
            # Chi ph√≠ giao d·ªãch
            turnover = np.abs(current_weights - target_weights).sum()
            cost = turnover * transaction_cost
            total_costs += cost
            
            # Tr·ª´ chi ph√≠
            rebal_value = rebal_value * (1 - cost)
            
            # Rebalance v·ªÅ target weights
            rebal_assets = target_weights * rebal_value
            
            rebalance_dates.append(dates[i])
            days_since_rebalance = 0
        
        rebal_values.append(rebal_value)
        
        # === BUY & HOLD ===
        bh_assets = bh_assets * (1 + daily_returns)
        bh_value = bh_assets.sum()
        bh_values.append(bh_value)
    
    return {
        'rebalanced_value': pd.Series(rebal_values, index=dates),
        'buy_hold_value': pd.Series(bh_values, index=dates),
        'rebalance_dates': rebalance_dates,
        'total_costs': total_costs,
        'num_rebalances': len(rebalance_dates)
    }

# --- [M·ªöI] H√†m export Excel ---
def export_to_excel(weights_df, summary_table, returns_df, price_pivot):
    """Xu·∫•t k·∫øt qu·∫£ ra file Excel"""
    output = BytesIO()
    with pd.ExcelWriter(output, engine='xlsxwriter') as writer:
        # Sheet 1: T·ª∑ tr·ªçng t·ªëi ∆∞u
        weights_df.to_excel(writer, sheet_name='T·ª∑ tr·ªçng t·ªëi ∆∞u')
        
        # Sheet 2: B·∫£ng t·ªïng k·∫øt
        summary_table.to_excel(writer, sheet_name='T·ªïng k·∫øt hi·ªáu su·∫•t')
        
        # Sheet 3: Returns
        returns_df.to_excel(writer, sheet_name='T·ª∑ su·∫•t sinh l·ªùi')
        
        # Sheet 4: Prices
        price_pivot.to_excel(writer, sheet_name='Gi√° ƒë√≥ng c·ª≠a')
        
        # Sheet 5: H∆∞·ªõng d·∫´n
        instructions = pd.DataFrame({
            'Sheet': ['T·ª∑ tr·ªçng t·ªëi ∆∞u', 'T·ªïng k·∫øt hi·ªáu su·∫•t', 'T·ª∑ su·∫•t sinh l·ªùi', 'Gi√° ƒë√≥ng c·ª≠a'],
            'M√¥ t·∫£': [
                'Ph√¢n b·ªï t·ª∑ tr·ªçng % cho 3 chi·∫øn l∆∞·ª£c ƒë·∫ßu t∆∞',
                'C√°c ch·ªâ s·ªë ƒë√°nh gi√° hi·ªáu su·∫•t (CAGR, Sharpe, Drawdown...)',
                'T·ª∑ su·∫•t sinh l·ªùi h√†ng ng√†y c·ªßa t·ª´ng c·ªï phi·∫øu',
                'Gi√° ƒë√≥ng c·ª≠a ƒëi·ªÅu ch·ªânh c·ªßa t·ª´ng c·ªï phi·∫øu'
            ]
        })
        instructions.to_excel(writer, sheet_name='H∆∞·ªõng d·∫´n', index=False)
    
    return output.getvalue()

# --- [M·ªöI] Ph√¢n lo·∫°i Sector ---
SECTOR_MAPPING = {
    # Banking
    'ACB': 'Ng√¢n h√†ng', 'BID': 'Ng√¢n h√†ng', 'CTG': 'Ng√¢n h√†ng', 'HDB': 'Ng√¢n h√†ng',
    'LPB': 'Ng√¢n h√†ng', 'MBB': 'Ng√¢n h√†ng', 'SHB': 'Ng√¢n h√†ng', 'SSB': 'Ng√¢n h√†ng',
    'STB': 'Ng√¢n h√†ng', 'TCB': 'Ng√¢n h√†ng', 'TPB': 'Ng√¢n h√†ng', 'VCB': 'Ng√¢n h√†ng',
    'VIB': 'Ng√¢n h√†ng', 'VPB': 'Ng√¢n h√†ng',
    
    # Real Estate
    'VHM': 'B·∫•t ƒë·ªông s·∫£n', 'VIC': 'B·∫•t ƒë·ªông s·∫£n', 'VRE': 'B·∫•t ƒë·ªông s·∫£n',
    
    # Oil & Gas
    'GAS': 'D·∫ßu kh√≠', 'PLX': 'D·∫ßu kh√≠', 'GVR': 'D·∫ßu kh√≠',
    
    # Manufacturing
    'HPG': 'S·∫£n xu·∫•t', 'GVR': 'S·∫£n xu·∫•t',
    
    # Consumer
    'MSN': 'Ti√™u d√πng', 'MWG': 'Ti√™u d√πng', 'SAB': 'Ti√™u d√πng', 'VNM': 'Ti√™u d√πng',
    
    # Technology
    'FPT': 'C√¥ng ngh·ªá',
    
    # Insurance
    'BVH': 'B·∫£o hi·ªÉm',
    
    # Aviation
    'VJC': 'H√†ng kh√¥ng',
    
    # Securities
    'SSI': 'Ch·ª©ng kho√°n',
    
    # Mining
    'BCM': 'Khai kho√°ng'
}

def get_sector_allocation(weights_df):
    """T√≠nh ph√¢n b·ªï theo ng√†nh"""
    sector_data = {}
    for portfolio in weights_df.columns:
        sector_weights = {}
        for ticker, weight in weights_df[portfolio].items():
            if weight > 0.001:
                sector = SECTOR_MAPPING.get(ticker, 'Kh√°c')
                sector_weights[sector] = sector_weights.get(sector, 0) + weight
        sector_data[portfolio] = sector_weights
    return pd.DataFrame(sector_data).fillna(0)

# --- [M·ªöI] Machine Learning: D·ª± ƒëo√°n Return ---
def ml_predict_returns(returns_df, price_pivot):
    """S·ª≠ d·ª•ng Random Forest ƒë·ªÉ d·ª± ƒëo√°n returns"""
    predictions = {}
    feature_importance = {}
    
    for ticker in returns_df.columns:  # Train cho T·∫§T C·∫¢ c·ªï phi·∫øu
        try:
            # T·∫°o features
            df = pd.DataFrame()
            df['return'] = returns_df[ticker]
            df['return_lag1'] = df['return'].shift(1)
            df['return_lag2'] = df['return'].shift(2)
            df['return_lag3'] = df['return'].shift(3)
            df['return_ma5'] = df['return'].rolling(5).mean()
            df['return_ma20'] = df['return'].rolling(20).mean()
            df['volatility_20'] = df['return'].rolling(20).std()
            df = df.dropna()
            
            if len(df) < 100:
                continue
            
            # Train/Test split
            train_size = int(len(df) * 0.8)
            X_train = df.iloc[:train_size, 1:]
            y_train = df.iloc[:train_size, 0]
            X_test = df.iloc[train_size:, 1:]
            y_test = df.iloc[train_size:, 0]
            
            # Train model
            scaler = StandardScaler()
            X_train_scaled = scaler.fit_transform(X_train)
            X_test_scaled = scaler.transform(X_test)
            
            model = RandomForestRegressor(n_estimators=50, max_depth=5, random_state=42, n_jobs=-1)
            model.fit(X_train_scaled, y_train)
            
            # Predict
            y_pred = model.predict(X_test_scaled)
            
            predictions[ticker] = {
                'actual': y_test.values,
                'predicted': y_pred,
                'score': model.score(X_test_scaled, y_test)
            }
            
            feature_importance[ticker] = pd.Series(
                model.feature_importances_,
                index=X_train.columns
            )
        except:
            continue
    
    return predictions, feature_importance

# --- [M·ªöI] Clustering c·ªï phi·∫øu ---
def cluster_stocks(returns_df, n_clusters=3):
    """Ph√¢n nh√≥m c·ªï phi·∫øu theo ƒë·∫∑c t√≠nh"""
    # T√≠nh features cho m·ªói c·ªï phi·∫øu
    features = pd.DataFrame({
        'mean_return': returns_df.mean(),
        'volatility': returns_df.std(),
        'sharpe': returns_df.mean() / returns_df.std(),
        'skewness': returns_df.skew(),
        'kurtosis': returns_df.kurtosis()
    })
    
    # Chu·∫©n h√≥a
    scaler = StandardScaler()
    features_scaled = scaler.fit_transform(features)
    
    # Clustering
    kmeans = KMeans(n_clusters=n_clusters, random_state=42)
    clusters = kmeans.fit_predict(features_scaled)
    
    features['Cluster'] = clusters
    features['Ticker'] = features.index
    
    return features


# === GIAO DI·ªÜN STREAMLIT ===

# --- Sidebar (Thanh b√™n) ---
st.sidebar.header("C·∫•u h√¨nh Ph√¢n t√≠ch")

start_date_input = st.sidebar.date_input(
    'T·ª´ ng√†y', value=datetime(2018, 1, 1)
)
end_date_input = st.sidebar.date_input(
    'ƒê·∫øn ng√†y', value=datetime.now()
)

holding_period_tuple = st.sidebar.selectbox(
    'Th·ªùi h·∫°n (Scale):',
    options=[('1 nƒÉm', 252), ('6 th√°ng', 126), ('2 nƒÉm', 504), ('3 th√°ng', 63)],
    format_func=lambda x: x[0]
)
HE_SO_SCALE = holding_period_tuple[1]

risk_free_rate_pct = st.sidebar.number_input(
    'LS Phi r·ªßi ro (%):', value=4.0, step=0.1
)
RISK_FREE_RATE = risk_free_rate_pct / 100.0

N_SIMULATIONS = st.sidebar.number_input(
    'S·ªë l·∫ßn M√¥ ph·ªèng Monte Carlo:',
    min_value=1000, max_value=50000, value=5000, step=1000
)

# TH√äM: Gi·ªõi h·∫°n t·ª∑ tr·ªçng ƒë·ªÉ ƒëa d·∫°ng h√≥a
st.sidebar.subheader("‚öñÔ∏è ƒêa d·∫°ng h√≥a Danh m·ª•c")
max_weight_pct = st.sidebar.slider(
    'T·ª∑ tr·ªçng t·ªëi ƒëa m·ªói c·ªï phi·∫øu (%):', 
    min_value=5, max_value=100, value=30, step=5,
    help="Gi·ªõi h·∫°n t·ª∑ tr·ªçng ƒë·ªÉ tr√°nh t·∫≠p trung r·ªßi ro v√†o 1 c·ªï phi·∫øu"
)
MAX_WEIGHT = max_weight_pct / 100.0

min_stocks = st.sidebar.number_input(
    'S·ªë c·ªï phi·∫øu t·ªëi thi·ªÉu trong danh m·ª•c:',
    min_value=3, max_value=30, value=5, step=1,
    help="ƒê·∫£m b·∫£o danh m·ª•c c√≥ √≠t nh·∫•t X c·ªï phi·∫øu ƒë·ªÉ ƒëa d·∫°ng"
)

force_refresh_checkbox = st.sidebar.checkbox(
    'L√†m m·ªõi D·ªØ li·ªáu (B·ªè qua Cache & g·ªçi l·∫°i API)'
)

run_button = st.sidebar.button("B·∫ÆT ƒê·∫¶U PH√ÇN T√çCH", type="primary", use_container_width=True)

tickers_list = [
    'ACB', 'BCM', 'BID', 'BVH', 'CTG', 'FPT', 'GAS', 'GVR', 'HDB', 'HPG', 
    'LPB', 'MBB', 'MSN', 'MWG', 'PLX', 'SAB', 'SHB', 'SSB', 'SSI', 'STB', 
    'TCB', 'TPB', 'VCB', 'VHM', 'VIB', 'VIC', 'VJC', 'VNM', 'VPB', 'VRE'
]

# === QUY TR√åNH CH√çNH ===

if 'analysis_done' not in st.session_state:
    st.session_state.analysis_done = False

if run_button:
    st.session_state.analysis_done = False 
    
    # 1. T·∫£i D·ªØ li·ªáu
    with st.spinner("‚è≥ (1/7) ƒêang t·∫£i d·ªØ li·ªáu th√¥..."):
        start_time_str = start_date_input.strftime('%Y-%m-%d')
        end_time_str = end_date_input.strftime('%Y-%m-%d')
        
        raw_data = load_data(tickers_list, start_time_str, end_time_str, force_refresh_checkbox)
        
        if raw_data.empty:
            st.error("Kh√¥ng t·∫£i ƒë∆∞·ª£c d·ªØ li·ªáu. Vui l√≤ng th·ª≠ l·∫°i.")
            st.stop()
        
        st.session_state.raw_data = raw_data
        st.session_state.start_time_str = start_time_str
    
    # 2. T√≠nh T·ª∑ su·∫•t sinh l·ªùi
    with st.spinner("‚è≥ (2/7) ƒêang t√≠nh T·ª∑ su·∫•t sinh l·ªùi..."):
        raw_data.drop_duplicates(subset=['time', 'ticker'], keep='last', inplace=True)
        price_pivot = raw_data.pivot(index='time', columns='ticker', values='close')
        returns_df_raw = price_pivot.pct_change()
        returns_df = returns_df_raw.iloc[1:].dropna(how='all') 
        
        st.session_state.returns_df = returns_df
        st.session_state.price_pivot = price_pivot
    
    # 3. T√≠nh Stats
    with st.spinner("‚è≥ (3/7) ƒêang t√≠nh L·ª£i nhu·∫≠n K·ª≥ v·ªçng & Hi·ªáp ph∆∞∆°ng sai..."):
        expected_returns, cov_matrix = calculate_stats(returns_df, HE_SO_SCALE)
        
        valid_assets = expected_returns.dropna().index
        expected_returns = expected_returns[valid_assets]
        cov_matrix = cov_matrix.loc[valid_assets, valid_assets]
        
        st.session_state.expected_returns = expected_returns
        st.session_state.cov_matrix = cov_matrix
        st.session_state.returns_df = returns_df[valid_assets] 
        st.session_state.price_pivot = price_pivot[valid_assets]
    
    # 4. Ch·∫°y Monte Carlo V√Ä Efficient Frontier
    with st.spinner(f"‚è≥ (4/7) ƒêang ch·∫°y Monte Carlo & D·ª±ng ƒë∆∞·ªùng bi√™n hi·ªáu qu·∫£..."):
        sim_data_df = run_monte_carlo_sim(
            N_SIMULATIONS, 
            st.session_state.expected_returns, 
            st.session_state.cov_matrix, 
            RISK_FREE_RATE
        )
        st.session_state.sim_data_df = sim_data_df

        eff_frontier_df = calculate_theoretical_efficient_frontier(
            st.session_state.expected_returns, 
            st.session_state.cov_matrix, 
            RISK_FREE_RATE
        )
        st.session_state.eff_frontier_df = eff_frontier_df

    # 5. Ch·∫°y T·ªëi ∆∞u h√≥a (C√ì GI·ªöI H·∫†N ƒêA D·∫†NG H√ìA)
    with st.spinner("‚è≥ (5/7) ƒêang ch·∫°y T·ªëi ∆∞u h√≥a v·ªõi gi·ªõi h·∫°n ƒëa d·∫°ng..."):
        num_assets = len(st.session_state.expected_returns)
        args = (st.session_state.expected_returns, st.session_state.cov_matrix, RISK_FREE_RATE)
        
        # R√†ng bu·ªôc: T·ªïng = 1, T·ª∑ tr·ªçng <= MAX_WEIGHT, S·ªë c·ªï phi·∫øu >= min_stocks
        constraints = [
            {'type': 'eq', 'fun': lambda x: np.sum(x) - 1},  # T·ªïng = 100%
        ]
        bounds = tuple((0.0, MAX_WEIGHT) for _ in range(num_assets))  # Gi·ªõi h·∫°n t·ª´ng c·ªï phi·∫øu

        # 1. Min Risk v·ªõi ƒëa d·∫°ng h√≥a
        min_vol_guess_weights = sim_data_df.loc[sim_data_df['Risk'].idxmin()].values[3:3+num_assets]
        min_vol_guess_weights = np.clip(min_vol_guess_weights, 0, MAX_WEIGHT)
        min_vol_guess_weights /= min_vol_guess_weights.sum()
        
        opt_min_vol = minimize(minimize_portfolio_risk, min_vol_guess_weights, args=args,
                               method='SLSQP', bounds=bounds, constraints=constraints)
        min_vol_weights = opt_min_vol.x
        
        # ƒê·∫£m b·∫£o c√≥ ƒë·ªß s·ªë c·ªï phi·∫øu
        if np.sum(min_vol_weights > 0.001) < min_stocks:
            # Ph√¢n b·ªï ƒë·ªÅu cho top N c·ªï phi·∫øu c√≥ risk th·∫•p nh·∫•t
            top_n_idx = np.argsort(np.diag(st.session_state.cov_matrix))[:min_stocks]
            min_vol_weights = np.zeros(num_assets)
            min_vol_weights[top_n_idx] = 1.0 / min_stocks

        # 2. Max Sharpe v·ªõi ƒëa d·∫°ng h√≥a
        max_sharpe_guess_weights = sim_data_df.loc[sim_data_df['Sharpe'].idxmax()].values[3:3+num_assets]
        max_sharpe_guess_weights = np.clip(max_sharpe_guess_weights, 0, MAX_WEIGHT)
        max_sharpe_guess_weights /= max_sharpe_guess_weights.sum()
        
        opt_max_sharpe = minimize(minimize_negative_sharpe, max_sharpe_guess_weights, args=args,
                                  method='SLSQP', bounds=bounds, constraints=constraints)
        max_sharpe_weights = opt_max_sharpe.x
        
        if np.sum(max_sharpe_weights > 0.001) < min_stocks:
            top_n_idx = np.argsort(-st.session_state.expected_returns)[:min_stocks]
            max_sharpe_weights = np.zeros(num_assets)
            max_sharpe_weights[top_n_idx] = 1.0 / min_stocks
        
        # 3. Max Return v·ªõi ƒëa d·∫°ng h√≥a (kh√¥ng cho ph√©p 100% v√†o 1 c·ªï phi·∫øu)
        top_n_returns_idx = np.argsort(-st.session_state.expected_returns)[:min_stocks]
        max_ret_weights = np.zeros(num_assets)
        
        # Ph√¢n b·ªï theo t·ª∑ l·ªá l·ª£i nhu·∫≠n c·ªßa top stocks
        top_returns = st.session_state.expected_returns.values[top_n_returns_idx]
        top_returns = np.maximum(top_returns, 0)  # Ch·ªâ l·∫•y returns d∆∞∆°ng
        if top_returns.sum() > 0:
            max_ret_weights[top_n_returns_idx] = top_returns / top_returns.sum()
            max_ret_weights = np.clip(max_ret_weights, 0, MAX_WEIGHT)
            max_ret_weights /= max_ret_weights.sum()
        else:
            # N·∫øu kh√¥ng c√≥ returns d∆∞∆°ng, ph√¢n b·ªï ƒë·ªÅu
            max_ret_weights[top_n_returns_idx] = 1.0 / min_stocks

        st.session_state.optimal_weights_df = pd.DataFrame({
            'B·∫£o th·ªß (Min Risk)': min_vol_weights,
            'C√¢n b·∫±ng (Max Sharpe)': max_sharpe_weights,
            'M·∫°o hi·ªÉm (Max Return)': max_ret_weights
        }, index=st.session_state.expected_returns.index)
        
        st.session_state.optimal_weights_df.index.name = 'ticker'

        st.session_state.optimal_stats_dict = {
            'min_vol': get_portfolio_stats(min_vol_weights, *args),
            'max_sharpe': get_portfolio_stats(max_sharpe_weights, *args),
            'max_ret': get_portfolio_stats(max_ret_weights, *args)
        }
        
        # TH√äM: T√≠nh s·ªë c·ªï phi·∫øu th·ª±c t·∫ø trong m·ªói danh m·ª•c
        st.session_state.num_stocks = {
            'B·∫£o th·ªß (Min Risk)': np.sum(min_vol_weights > 0.001),
            'C√¢n b·∫±ng (Max Sharpe)': np.sum(max_sharpe_weights > 0.001),
            'M·∫°o hi·ªÉm (Max Return)': np.sum(max_ret_weights > 0.001)
        }

    # 6. Ch·∫°y Backtest
    with st.spinner("‚è≥ (6/7) ƒêang ch·∫°y Backtest..."):
        returns_min_vol, cum_min_vol = run_simple_backtest(st.session_state.returns_df, min_vol_weights)
        returns_max_sharpe, cum_max_sharpe = run_simple_backtest(st.session_state.returns_df, max_sharpe_weights)
        returns_max_ret, cum_max_ret = run_simple_backtest(st.session_state.returns_df, max_ret_weights)
        
        st.session_state.all_cumulative_df = pd.DataFrame({
            'B·∫£o th·ªß (Min Risk)': cum_min_vol,
            'C√¢n b·∫±ng (Max Sharpe)': cum_max_sharpe,
            'M·∫°o hi·ªÉm (Max Return)': cum_max_ret
        }).dropna()
        
        st.session_state.all_returns_df = pd.DataFrame({
            'B·∫£o th·ªß (Min Risk)': returns_min_vol,
            'C√¢n b·∫±ng (Max Sharpe)': returns_max_sharpe,
            'M·∫°o hi·ªÉm (Max Return)': returns_max_ret
        }).dropna()

    # 7. T√≠nh Metrics
    with st.spinner("‚è≥ (7/7) ƒêang t√≠nh to√°n Metrics hi·ªáu su·∫•t..."):
        metrics = ['T·ªïng L·ª£i nhu·∫≠n (Cumulative)', 'L·ª£i nhu·∫≠n TB NƒÉm (Annualized)', 
                   'R·ªßi ro NƒÉm (Annualized)', 'M·ª©c s·ª•t gi·∫£m T·ªëi ƒëa (Max Drawdown)', 
                   'Ch·ªâ s·ªë Sharpe (Historical)']
        summary_table = pd.DataFrame(index=metrics)
        
        for port_name in st.session_state.all_returns_df.columns:
            returns_series = st.session_state.all_returns_df[port_name]
            summary_table.loc['T·ªïng L·ª£i nhu·∫≠n (Cumulative)', port_name] = qs.stats.comp(returns_series)
            summary_table.loc['L·ª£i nhu·∫≠n TB NƒÉm (Annualized)', port_name] = qs.stats.cagr(returns_series)
            summary_table.loc['R·ªßi ro NƒÉm (Annualized)', port_name] = qs.stats.volatility(returns_series)
            summary_table.loc['M·ª©c s·ª•t gi·∫£m T·ªëi ƒëa (Max Drawdown)', port_name] = qs.stats.max_drawdown(returns_series)
            summary_table.loc['Ch·ªâ s·ªë Sharpe (Historical)', port_name] = qs.stats.sharpe(returns_series, rf=RISK_FREE_RATE)
        
        st.session_state.summary_table = summary_table
    
    st.session_state.analysis_done = True
    st.success("üéâ Ph√¢n t√≠ch ho√†n t·∫•t! Xem k·∫øt qu·∫£ b√™n d∆∞·ªõi.")

# === HI·ªÇN TH·ªä K·∫æT QU·∫¢ ===

if st.session_state.analysis_done:
    
    # TH√äM: N√∫t Export ·ªü ƒë·∫ßu
    st.markdown("---")
    col_export1, col_export2, col_export3 = st.columns([1, 1, 2])
    
    with col_export1:
        # Export Excel
        excel_data = export_to_excel(
            st.session_state.optimal_weights_df,
            st.session_state.summary_table,
            st.session_state.returns_df,
            st.session_state.price_pivot
        )
        st.download_button(
            label="üì• T·∫£i xu·ªëng Excel",
            data=excel_data,
            file_name=f"Portfolio_Analysis_{datetime.now().strftime('%Y%m%d')}.xlsx",
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
        )
    
    with col_export2:
        # Export CSV
        csv_data = st.session_state.optimal_weights_df.to_csv()
        st.download_button(
            label="üì• T·∫£i xu·ªëng CSV",
            data=csv_data,
            file_name=f"Portfolio_Weights_{datetime.now().strftime('%Y%m%d')}.csv",
            mime="text/csv"
        )
    
    with col_export3:
        st.info("üíæ T·∫£i xu·ªëng k·∫øt qu·∫£ ph√¢n t√≠ch ƒë·ªÉ l∆∞u tr·ªØ ho·∫∑c b√°o c√°o")
    
    st.markdown("---")
    
    # TH√äM: Th√™m tab m·ªõi
    tab1, tab2, tab3, tab4, tab5 = st.tabs([
        "üìä D·ªØ li·ªáu & T∆∞∆°ng quan", 
        "üéØ T·ª∑ tr·ªçng & ƒê∆∞·ªùng bi√™n", 
        "üìà Backtest & Hi·ªáu su·∫•t",
        "üõ°Ô∏è R·ªßi ro & ƒêa d·∫°ng",
        "üè¢ Ph√¢n t√≠ch Ng√†nh & ML"
    ])

    # Tab 1: D·ªØ li·ªáu Th√¥ & T∆∞∆°ng quan
    with tab1:
        st.header("üìä D·ªØ li·ªáu Th√¥ & Ph√¢n t√≠ch T∆∞∆°ng quan")
        
        # TH√äM: Th·ªëng k√™ m√¥ t·∫£
        st.subheader("Th·ªëng k√™ M√¥ t·∫£ T·ª∑ su·∫•t Sinh l·ªùi")
        returns_df = st.session_state.returns_df
        desc_stats = returns_df.describe().T
        desc_stats['skew'] = returns_df.skew()
        desc_stats['kurtosis'] = returns_df.kurtosis()
        desc_stats_styled = desc_stats.style.format({
            'mean': '{:.4f}',
            'std': '{:.4f}',
            'min': '{:.4f}',
            '25%': '{:.4f}',
            '50%': '{:.4f}',
            '75%': '{:.4f}',
            'max': '{:.4f}',
            'skew': '{:.2f}',
            'kurtosis': '{:.2f}'
        })
        st.dataframe(desc_stats_styled, use_container_width=True)
        
        # TH√äM: Bi·ªÉu ƒë·ªì ph√¢n ph·ªëi l·ª£i nhu·∫≠n
        st.subheader("Ph√¢n ph·ªëi L·ª£i nhu·∫≠n H√†ng ng√†y (Top 10 c·ªï phi·∫øu)")
        top_10_tickers = st.session_state.expected_returns.nlargest(10).index.tolist()
        fig_dist = go.Figure()
        for ticker in top_10_tickers:
            fig_dist.add_trace(go.Histogram(
                x=returns_df[ticker].dropna(),
                name=ticker,
                opacity=0.6,
                nbinsx=50
            ))
        fig_dist.update_layout(
            title='Ph√¢n ph·ªëi L·ª£i nhu·∫≠n H√†ng ng√†y (Top 10 c·ªï phi·∫øu theo Return k·ª≥ v·ªçng)',
            xaxis_title='T·ª∑ su·∫•t sinh l·ªùi',
            yaxis_title='T·∫ßn su·∫•t',
            barmode='overlay',
            template='plotly_dark',
            height=500
        )
        st.plotly_chart(fig_dist, use_container_width=True)
        
        st.subheader("Heatmap Ma tr·∫≠n T∆∞∆°ng quan")
        correlation_matrix = returns_df.corr()
        labels = correlation_matrix.columns
        fig_heatmap = go.Figure(data=go.Heatmap(
            z=correlation_matrix.values, x=labels, y=labels,
            colorscale='RdBu_r', zmin=-1, zmax=1,
            hoverongaps=False,
            text=correlation_matrix.values,
            texttemplate='%{text:.2f}',
            textfont={"size": 8}
        ))
        fig_heatmap.update_layout(
            title='Heatmap Ma tr·∫≠n T∆∞∆°ng quan (VN30)', 
            template='plotly_dark',
            height=800, width=900,
            yaxis_autorange='reversed'
        )
        st.plotly_chart(fig_heatmap, use_container_width=True)
        
        # TH√äM: Bi·ªÉu ƒë·ªì c·∫∑p t∆∞∆°ng quan cao nh·∫•t
        st.subheader("Top 10 c·∫∑p c·ªï phi·∫øu c√≥ T∆∞∆°ng quan cao nh·∫•t")
        corr_pairs = []
        for i in range(len(correlation_matrix.columns)):
            for j in range(i+1, len(correlation_matrix.columns)):
                corr_pairs.append({
                    'C·∫∑p': f"{correlation_matrix.columns[i]} - {correlation_matrix.columns[j]}",
                    'T∆∞∆°ng quan': correlation_matrix.iloc[i, j]
                })
        corr_pairs_df = pd.DataFrame(corr_pairs).sort_values('T∆∞∆°ng quan', ascending=False).head(10)
        fig_corr_pairs = px.bar(
            corr_pairs_df, x='T∆∞∆°ng quan', y='C·∫∑p', orientation='h',
            title='Top 10 c·∫∑p c·ªï phi·∫øu c√≥ T∆∞∆°ng quan cao nh·∫•t',
            text='T∆∞∆°ng quan'
        )
        fig_corr_pairs.update_traces(texttemplate='%{text:.3f}', textposition='outside')
        fig_corr_pairs.update_layout(template='plotly_dark', height=500)
        st.plotly_chart(fig_corr_pairs, use_container_width=True)
        
        st.subheader("D·ªØ li·ªáu Gi√° ƒê√≥ng c·ª≠a (Pivot - 10 d√≤ng cu·ªëi)")
        st.dataframe(st.session_state.price_pivot.tail(10))
        
        st.subheader("D·ªØ li·ªáu T·ª∑ su·∫•t sinh l·ªùi (H√†ng ng√†y - 10 d√≤ng cu·ªëi)")
        st.dataframe(st.session_state.returns_df.tail(10))

    # Tab 2: Ph√¢n b·ªï T·ª∑ tr·ªçng & ƒê∆∞·ªùng bi√™n
    with tab2:
        st.header("üéØ Ph√¢n b·ªï T·ª∑ tr·ªçng & ƒê∆∞·ªùng bi√™n Hi·ªáu qu·∫£")
        
        # TH√äM: Hi·ªÉn th·ªã th√¥ng tin ƒëa d·∫°ng h√≥a
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("üõ°Ô∏è T·ª∑ tr·ªçng t·ªëi ƒëa", f"{MAX_WEIGHT*100:.0f}%")
        with col2:
            st.metric("üìä S·ªë c·ªï phi·∫øu t·ªëi thi·ªÉu", f"{min_stocks} c·ªï phi·∫øu")
        with col3:
            avg_stocks = np.mean(list(st.session_state.num_stocks.values()))
            st.metric("üìà S·ªë c·ªï phi·∫øu TB trong danh m·ª•c", f"{avg_stocks:.1f} c·ªï phi·∫øu")
        
        st.divider()
        
        # --- PH·∫¶N 1: B·∫¢NG S·ªê LI·ªÜU ---
        st.subheader("üìã Ph√¢n b·ªï T·ª∑ tr·ªçng T·ªëi ∆∞u")
        df_weights = st.session_state.optimal_weights_df
        df_weights_display = df_weights[(df_weights > 0.001).any(axis=1)].copy()
        
        # TH√äM: S·ªë l∆∞·ª£ng c·ªï phi·∫øu trong m·ªói danh m·ª•c
        num_stocks_row = pd.DataFrame({
            col: [f"{st.session_state.num_stocks[col]} c·ªï phi·∫øu"] 
            for col in df_weights.columns
        }, index=['S·ªë l∆∞·ª£ng CP'])
        
        st.info(f"**Nguy√™n t·∫Øc ƒëa d·∫°ng h√≥a**: M·ªói c·ªï phi·∫øu kh√¥ng v∆∞·ª£t qu√° {MAX_WEIGHT*100:.0f}%, ƒë·∫£m b·∫£o √≠t nh·∫•t {min_stocks} c·ªï phi·∫øu trong danh m·ª•c")
        
        # Hi·ªÉn th·ªã s·ªë l∆∞·ª£ng c·ªï phi·∫øu
        st.dataframe(num_stocks_row, use_container_width=True)
        
        df_weights_styled = df_weights_display.style.format("{:.2%}").background_gradient(
            cmap='RdYlGn', axis=0, vmin=0, vmax=MAX_WEIGHT
        )
        st.dataframe(df_weights_styled, use_container_width=True)
        
        st.divider()
        
        # --- PH·∫¶N 2: BI·ªÇU ƒê·ªí TR√íN T·ª∂ TR·ªåNG ---
        st.subheader("Bi·ªÉu ƒë·ªì Tr√≤n - C∆° c·∫•u Danh m·ª•c")
        selected_portfolio = st.selectbox(
            "Ch·ªçn danh m·ª•c ƒë·ªÉ xem chi ti·∫øt:",
            options=df_weights.columns.tolist()
        )
        
        weights_selected = df_weights[selected_portfolio]
        weights_selected = weights_selected[weights_selected > 0.001].sort_values(ascending=False)
        
        fig_pie = go.Figure(data=[go.Pie(
            labels=weights_selected.index,
            values=weights_selected.values,
            hole=0.4,
            textposition='auto',
            textinfo='label+percent',
            marker=dict(line=dict(color='#000000', width=2))
        )])
        fig_pie.update_layout(
            title=f'C∆° c·∫•u Danh m·ª•c: {selected_portfolio}',
            template='plotly_dark',
            height=600,
            annotations=[dict(text=f'{len(weights_selected)}<br>c·ªï phi·∫øu', 
                            x=0.5, y=0.5, font_size=20, showarrow=False)]
        )
        st.plotly_chart(fig_pie, use_container_width=True)
        
        st.divider()
        
        # --- PH·∫¶N 3: BI·ªÇU ƒê·ªí T·ª∂ TR·ªåNG SO S√ÅNH ---
        st.subheader("Bi·ªÉu ƒë·ªì T·ª∑ tr·ªçng - So s√°nh 3 Danh m·ª•c")
        df_plot = df_weights[(df_weights > 0.001).any(axis=1)].copy()
        
        df_plot_long = df_plot.reset_index().melt(
            id_vars='ticker', 
            var_name='Danh m·ª•c', 
            value_name='T·ª∑ tr·ªçng'
        )
        df_plot_long.rename(columns={'ticker': 'M√£ CP'}, inplace=True)
        
        fig_bars = px.bar(
            df_plot_long, x='Danh m·ª•c', y='T·ª∑ tr·ªçng', color='M√£ CP',
            text_auto='.1%',
            title='Ph√¢n b·ªï T·ª∑ tr·ªçng T·ªëi ∆∞u theo 3 Kh·∫©u v·ªã R·ªßi ro'
        )
        fig_bars.update_layout(template='plotly_dark', yaxis_tickformat='.0%', height=600)
        fig_bars.add_hline(y=MAX_WEIGHT, line_dash="dash", line_color="red", 
                          annotation_text=f"Gi·ªõi h·∫°n {MAX_WEIGHT*100:.0f}%")
        st.plotly_chart(fig_bars, use_container_width=True)
            
        st.divider()

        # --- PH·∫¶N 4: BI·ªÇU ƒê·ªí ƒê∆Ø·ªúNG BI√äN HI·ªÜU QU·∫¢ ---
        st.subheader("ƒê∆∞·ªùng bi√™n Hi·ªáu qu·∫£ To√†n di·ªán (c√≥ CAL)")
        
        # FIX: TH√äM render_mode='svg' ƒê·ªÇ S·ª¨A L·ªñI WEBGL
        sim_data_df = st.session_state.sim_data_df
        fig = px.scatter(
            sim_data_df, x='Risk', y='Return', color='Sharpe',
            color_continuous_scale='Viridis',
            hover_data={col: ':.2%' for col in sim_data_df.columns if col not in ['Risk', 'Return', 'Sharpe']} | {'Risk': ':.2%','Return': ':.2%','Sharpe': ':.2f'},
            title=f'ƒê∆∞·ªùng bi√™n Hi·ªáu qu·∫£ - {N_SIMULATIONS} danh m·ª•c (Rf={RISK_FREE_RATE:.1%}, Max Weight={MAX_WEIGHT:.0%})',
            render_mode='svg' # <--- ƒê√ÇY L√Ä FIX QUAN TR·ªåNG
        )
        
        # V·∫Ω ƒë∆∞·ªùng l√Ω thuy·∫øt
        eff_df = st.session_state.eff_frontier_df
        fig.add_trace(go.Scatter(
            x=eff_df['Risk'], y=eff_df['Return'], mode='lines', 
            line=dict(color='white', width=3, dash='dash'),
            name='ƒê∆∞·ªùng bi√™n Hi·ªáu qu·∫£ (L√Ω thuy·∫øt)'
        ))
        
        # V·∫Ω c√°c ƒëi·ªÉm t·ªëi ∆∞u
        stats_dict = st.session_state.optimal_stats_dict
        stats_min_vol = stats_dict['min_vol']
        stats_max_sharpe = stats_dict['max_sharpe']
        stats_max_ret = stats_dict['max_ret']
        
        fig.add_trace(go.Scatter(x=[stats_min_vol[1]], y=[stats_min_vol[0]], mode='markers', 
                                marker=dict(color='white', size=20, symbol='star', 
                                          line=dict(color='black', width=2)), 
                                name='B·∫£o th·ªß (Min Risk)',
                                hovertemplate='<b>B·∫£o th·ªß</b><br>Risk: %{x:.2%}<br>Return: %{y:.2%}'))
        fig.add_trace(go.Scatter(x=[stats_max_sharpe[1]], y=[stats_max_sharpe[0]], mode='markers', 
                                marker=dict(color='cyan', size=20, symbol='star', 
                                          line=dict(color='black', width=2)), 
                                name='C√¢n b·∫±ng (Max Sharpe)',
                                hovertemplate='<b>C√¢n b·∫±ng</b><br>Risk: %{x:.2%}<br>Return: %{y:.2%}'))
        fig.add_trace(go.Scatter(x=[stats_max_ret[1]], y=[stats_max_ret[0]], mode='markers', 
                                marker=dict(color='red', size=20, symbol='star', 
                                          line=dict(color='black', width=2)), 
                                name='M·∫°o hi·ªÉm (Max Return)',
                                hovertemplate='<b>M·∫°o hi·ªÉm</b><br>Risk: %{x:.2%}<br>Return: %{y:.2%}'))
        
        # V·∫Ω ƒê∆∞·ªùng CAL
        sharpe_risk = stats_max_sharpe[1]
        sharpe_return = stats_max_sharpe[0]
        x_cal = [0, sharpe_risk * 1.5] 
        y_cal = [RISK_FREE_RATE, (sharpe_return - RISK_FREE_RATE) / (sharpe_risk + 1e-9) * (sharpe_risk * 1.5) + RISK_FREE_RATE]
        fig.add_trace(go.Scatter(x=x_cal, y=y_cal, mode='lines', 
                                line=dict(color='lime', width=3, dash='dash'), 
                                name='ƒê∆∞·ªùng Ph√¢n b·ªï V·ªën (CAL)'))

        fig.update_layout(
            height=800,
            xaxis_tickformat='.1%', yaxis_tickformat='.1%',
            legend=dict(orientation="h", yanchor="bottom", y=-0.15, xanchor="center", x=0.5),
            margin=dict(b=120)
        )
        st.plotly_chart(fig, use_container_width=True)
        
        # TH√äM: B·∫£ng so s√°nh 3 danh m·ª•c
        st.subheader("So s√°nh Ch·ªâ s·ªë c·ªßa 3 Danh m·ª•c T·ªëi ∆∞u")
        comparison_df = pd.DataFrame({
            'B·∫£o th·ªß (Min Risk)': [
                f"{stats_min_vol[0]:.2%}", 
                f"{stats_min_vol[1]:.2%}", 
                f"{stats_min_vol[2]:.2f}",
                f"{st.session_state.num_stocks['B·∫£o th·ªß (Min Risk)']} c·ªï phi·∫øu",
                f"{df_weights['B·∫£o th·ªß (Min Risk)'].max():.2%}"
            ],
            'C√¢n b·∫±ng (Max Sharpe)': [
                f"{stats_max_sharpe[0]:.2%}", 
                f"{stats_max_sharpe[1]:.2%}", 
                f"{stats_max_sharpe[2]:.2f}",
                f"{st.session_state.num_stocks['C√¢n b·∫±ng (Max Sharpe)']} c·ªï phi·∫øu",
                f"{df_weights['C√¢n b·∫±ng (Max Sharpe)'].max():.2%}"
            ],
            'M·∫°o hi·ªÉm (Max Return)': [
                f"{stats_max_ret[0]:.2%}", 
                f"{stats_max_ret[1]:.2%}", 
                f"{stats_max_ret[2]:.2f}",
                f"{st.session_state.num_stocks['M·∫°o hi·ªÉm (Max Return)']} c·ªï phi·∫øu",
                f"{df_weights['M·∫°o hi·ªÉm (Max Return)'].max():.2%}"
            ]
        }, index=['L·ª£i nhu·∫≠n K·ª≥ v·ªçng', 'R·ªßi ro (ƒê·ªô l·ªách chu·∫©n)', 'Ch·ªâ s·ªë Sharpe', 'S·ªë l∆∞·ª£ng CP', 'T·ª∑ tr·ªçng CP l·ªõn nh·∫•t'])
        st.dataframe(comparison_df, use_container_width=True)

    # Tab 3: K·∫øt qu·∫£ Backtest
    with tab3:
        st.header("üìà K·∫øt qu·∫£ Backtest & Hi·ªáu su·∫•t")
        
        st.subheader("B·∫£ng T·ªïng k·∫øt Ch·ªâ s·ªë Hi·ªáu su·∫•t")
        summary_table = st.session_state.summary_table
        percent_rows = summary_table.index.difference(['Ch·ªâ s·ªë Sharpe (Historical)'])
        number_row = pd.Index(['Ch·ªâ s·ªë Sharpe (Historical)'])
        styler = summary_table.style
        styler.format('{:,.2%}', subset=(percent_rows, slice(None)))
        styler.format('{:,.2f}', subset=(number_row, slice(None)))
        
        # TH√äM: Highlight gi√° tr·ªã t·ªët nh·∫•t
        styler.highlight_max(axis=1, color='lightgreen', subset=pd.IndexSlice[['T·ªïng L·ª£i nhu·∫≠n (Cumulative)', 'L·ª£i nhu·∫≠n TB NƒÉm (Annualized)', 'Ch·ªâ s·ªë Sharpe (Historical)'], :])
        styler.highlight_min(axis=1, color='lightgreen', subset=pd.IndexSlice[['R·ªßi ro NƒÉm (Annualized)', 'M·ª©c s·ª•t gi·∫£m T·ªëi ƒëa (Max Drawdown)'], :])
        
        st.dataframe(styler, use_container_width=True)
        
        st.divider()
        
        # TH√äM: C√°c metrics n·ªïi b·∫≠t
        col1, col2, col3 = st.columns(3)
        with col1:
            best_return = summary_table.loc['L·ª£i nhu·∫≠n TB NƒÉm (Annualized)'].idxmax()
            best_return_val = summary_table.loc['L·ª£i nhu·∫≠n TB NƒÉm (Annualized)', best_return]
            st.metric("üèÜ Danh m·ª•c c√≥ Return cao nh·∫•t", best_return, f"{best_return_val:.2%}")
        
        with col2:
            best_sharpe = summary_table.loc['Ch·ªâ s·ªë Sharpe (Historical)'].idxmax()
            best_sharpe_val = summary_table.loc['Ch·ªâ s·ªë Sharpe (Historical)', best_sharpe]
            st.metric("‚öñÔ∏è Danh m·ª•c c√≥ Sharpe t·ªët nh·∫•t", best_sharpe, f"{best_sharpe_val:.2f}")
        
        with col3:
            lowest_risk = summary_table.loc['R·ªßi ro NƒÉm (Annualized)'].idxmin()
            lowest_risk_val = summary_table.loc['R·ªßi ro NƒÉm (Annualized)', lowest_risk]
            st.metric("üõ°Ô∏è Danh m·ª•c c√≥ Risk th·∫•p nh·∫•t", lowest_risk, f"{lowest_risk_val:.2%}")
        
        st.divider()
        
        st.subheader(f"So s√°nh Hi·ªáu qu·∫£ TƒÉng tr∆∞·ªüng (T·ª´ {st.session_state.start_time_str})")
        fig_backtest = px.line(
            st.session_state.all_cumulative_df, 
            title=f'So s√°nh Hi·ªáu qu·∫£ TƒÉng tr∆∞·ªüng (T·ª´ {st.session_state.start_time_str})'
        )
        fig_backtest.update_layout(
            template='plotly_dark', 
            yaxis_title='Gi√° tr·ªã Danh m·ª•c (B·∫Øt ƒë·∫ßu t·ª´ 1.0)', 
            legend_title='Danh m·ª•c',
            yaxis_tickformat='.2f',
            height=600,
            hovermode='x unified'
        )
        st.plotly_chart(fig_backtest, use_container_width=True)
        
        # TH√äM: Bi·ªÉu ƒë·ªì Drawdown
        st.subheader("Ph√¢n t√≠ch Drawdown (M·ª©c s·ª•t gi·∫£m)")
        drawdown_data = {}
        for col in st.session_state.all_cumulative_df.columns:
            cumulative = st.session_state.all_cumulative_df[col]
            running_max = cumulative.cummax()
            drawdown = (cumulative - running_max) / running_max
            drawdown_data[col] = drawdown
        
        drawdown_df = pd.DataFrame(drawdown_data)
        fig_drawdown = px.line(
            drawdown_df,
            title='Ph√¢n t√≠ch Drawdown theo th·ªùi gian'
        )
        fig_drawdown.update_layout(
            template='plotly_dark',
            yaxis_title='Drawdown',
            yaxis_tickformat='.1%',
            height=500,
            hovermode='x unified'
        )
        fig_drawdown.add_hline(y=0, line_dash="dash", line_color="white", opacity=0.5)
        st.plotly_chart(fig_drawdown, use_container_width=True)
        
        st.divider()
        
        st.subheader("Ph√¢n t√≠ch Chi ti·∫øt Hi·ªáu su·∫•t Backtest")
        port_names = summary_table.columns
        fig_metrics = make_subplots(
            rows=3, cols=1, shared_xaxes=True, vertical_spacing=0.1,
            subplot_titles=("So s√°nh L·ª£i nhu·∫≠n", "So s√°nh R·ªßi ro", "So s√°nh T·ª∑ l·ªá (Sharpe)")
        )
        # 1. L·ª£i nhu·∫≠n
        return_metrics = ['T·ªïng L·ª£i nhu·∫≠n (Cumulative)', 'L·ª£i nhu·∫≠n TB NƒÉm (Annualized)']
        for metric in return_metrics:
            fig_metrics.add_trace(go.Bar(
                x=port_names, y=summary_table.loc[metric], text=summary_table.loc[metric],
                texttemplate='%{y:.2%}', name=metric
            ), row=1, col=1)
        # 2. R·ªßi ro
        risk_metrics = ['R·ªßi ro NƒÉm (Annualized)', 'M·ª©c s·ª•t gi·∫£m T·ªëi ƒëa (Max Drawdown)']
        for metric in risk_metrics:
            fig_metrics.add_trace(go.Bar(
                x=port_names, y=summary_table.loc[metric], text=summary_table.loc[metric],
                texttemplate='%{y:.2%}', name=metric
            ), row=2, col=1)
        # 3. Sharpe
        sharpe_metric = 'Ch·ªâ s·ªë Sharpe (Historical)'
        fig_metrics.add_trace(go.Bar(
            x=port_names, y=summary_table.loc[sharpe_metric], text=summary_table.loc[sharpe_metric],
            texttemplate='%{y:.2f}', name=sharpe_metric, marker_color='cyan'
        ), row=3, col=1)
        
        fig_metrics.update_layout(height=1000, template='plotly_dark', barmode='group')
        fig_metrics.update_yaxes(title_text='L·ª£i nhu·∫≠n', tickformat='.0%', row=1, col=1)
        fig_metrics.update_yaxes(title_text='R·ªßi ro', tickformat='.0%', row=2, col=1)
        fig_metrics.update_yaxes(title_text='T·ª∑ l·ªá', tickformat='.2f', row=3, col=1)
        st.plotly_chart(fig_metrics, use_container_width=True)
    
    # Tab 4: Ph√¢n t√≠ch R·ªßi ro & ƒêa d·∫°ng
    with tab4:
        st.header("üõ°Ô∏è Ph√¢n t√≠ch R·ªßi ro & ƒêa d·∫°ng h√≥a")
        
        st.info(f"""
        **üí° Nguy√™n t·∫Øc "Kh√¥ng b·ªè h·∫øt tr·ª©ng v√†o 1 gi·ªè":**
        - T·ª∑ tr·ªçng t·ªëi ƒëa m·ªói c·ªï phi·∫øu: **{MAX_WEIGHT*100:.0f}%**
        - S·ªë c·ªï phi·∫øu t·ªëi thi·ªÉu: **{min_stocks} c·ªï phi·∫øu**
        - M·ª•c ti√™u: Gi·∫£m r·ªßi ro t·∫≠p trung, tƒÉng t√≠nh ·ªïn ƒë·ªãnh c·ªßa danh m·ª•c
        """)
        
        st.divider()
        
        # TH√äM: Herfindahl Index - Ch·ªâ s·ªë t·∫≠p trung
        st.subheader("Ch·ªâ s·ªë T·∫≠p trung (Herfindahl Index)")
        st.markdown("""
        **Herfindahl Index** ƒëo l∆∞·ªùng m·ª©c ƒë·ªô t·∫≠p trung c·ªßa danh m·ª•c:
        - Gi√° tr·ªã g·∫ßn 1: T·∫≠p trung cao (r·ªßi ro)
        - Gi√° tr·ªã g·∫ßn 0: ƒêa d·∫°ng h√≥a t·ªët (√≠t r·ªßi ro)
        """)
        
        herfindahl_data = {}
        for col in st.session_state.optimal_weights_df.columns:
            weights = st.session_state.optimal_weights_df[col]
            herfindahl = (weights ** 2).sum()
            herfindahl_data[col] = herfindahl
        
        herfindahl_df = pd.DataFrame({
            'Danh m·ª•c': list(herfindahl_data.keys()),
            'Herfindahl Index': list(herfindahl_data.values())
        })
        
        fig_herfindahl = px.bar(
            herfindahl_df, x='Danh m·ª•c', y='Herfindahl Index',
            title='Ch·ªâ s·ªë Herfindahl - M·ª©c ƒë·ªô T·∫≠p trung Danh m·ª•c',
            text='Herfindahl Index',
            color='Herfindahl Index',
            color_continuous_scale='RdYlGn_r'
        )
        fig_herfindahl.update_traces(texttemplate='%{text:.3f}', textposition='outside')
        fig_herfindahl.update_layout(template='plotly_dark', height=500, showlegend=False)
        fig_herfindahl.add_hline(y=1/min_stocks, line_dash="dash", line_color="yellow",
                                annotation_text=f"Ph√¢n b·ªï ƒë·ªÅu {min_stocks} CP")
        st.plotly_chart(fig_herfindahl, use_container_width=True)
        
        st.divider()
        
        # TH√äM: Contribution to Risk (VaR)
        st.subheader("ƒê√≥ng g√≥p R·ªßi ro c·ªßa t·ª´ng C·ªï phi·∫øu")
        selected_portfolio_risk = st.selectbox(
            "Ch·ªçn danh m·ª•c ƒë·ªÉ ph√¢n t√≠ch:",
            options=st.session_state.optimal_weights_df.columns.tolist(),
            key='risk_analysis'
        )
        
        weights = st.session_state.optimal_weights_df[selected_portfolio_risk].values
        cov_matrix = st.session_state.cov_matrix
        
        # T√≠nh Marginal Contribution to Risk
        portfolio_variance = np.dot(weights.T, np.dot(cov_matrix, weights))
        portfolio_std = np.sqrt(portfolio_variance)
        
        marginal_contrib = np.dot(cov_matrix, weights) / portfolio_std
        contrib_to_risk = weights * marginal_contrib
        contrib_to_risk_pct = contrib_to_risk / contrib_to_risk.sum()
        
        risk_contrib_df = pd.DataFrame({
            'C·ªï phi·∫øu': st.session_state.optimal_weights_df.index,
            'T·ª∑ tr·ªçng': weights,
            'ƒê√≥ng g√≥p R·ªßi ro (%)': contrib_to_risk_pct
        }).sort_values('ƒê√≥ng g√≥p R·ªßi ro (%)', ascending=False).head(15)
        
        fig_risk_contrib = go.Figure()
        fig_risk_contrib.add_trace(go.Bar(
            x=risk_contrib_df['C·ªï phi·∫øu'],
            y=risk_contrib_df['T·ª∑ tr·ªçng'],
            name='T·ª∑ tr·ªçng',
            marker_color='lightblue',
            yaxis='y',
            offsetgroup=1
        ))
        fig_risk_contrib.add_trace(go.Bar(
            x=risk_contrib_df['C·ªï phi·∫øu'],
            y=risk_contrib_df['ƒê√≥ng g√≥p R·ªßi ro (%)'],
            name='ƒê√≥ng g√≥p R·ªßi ro',
            marker_color='salmon',
            yaxis='y',
            offsetgroup=2
        ))
        
        fig_risk_contrib.update_layout(
            title=f'So s√°nh T·ª∑ tr·ªçng vs ƒê√≥ng g√≥p R·ªßi ro - {selected_portfolio_risk}',
            xaxis_title='C·ªï phi·∫øu',
            yaxis_title='Gi√° tr·ªã (%)',
            template='plotly_dark',
            height=600,
            barmode='group',
            yaxis_tickformat='.1%'
        )
        st.plotly_chart(fig_risk_contrib, use_container_width=True)
        
        st.markdown("""
        **Gi·∫£i th√≠ch:**
        - **T·ª∑ tr·ªçng**: T·ª∑ l·ªá % v·ªën ƒë·∫ßu t∆∞ v√†o m·ªói c·ªï phi·∫øu
        - **ƒê√≥ng g√≥p R·ªßi ro**: % r·ªßi ro m√† m·ªói c·ªï phi·∫øu ƒë√≥ng g√≥p v√†o t·ªïng r·ªßi ro danh m·ª•c
        - N·∫øu ƒê√≥ng g√≥p R·ªßi ro >> T·ª∑ tr·ªçng ‚Üí C·ªï phi·∫øu n√†y c√≥ t∆∞∆°ng quan cao v·ªõi c√°c c·ªï phi·∫øu kh√°c
        """)
        
        st.divider()
        
        # TH√äM: Effective Number of Assets
        st.subheader("S·ªë l∆∞·ª£ng C·ªï phi·∫øu Hi·ªáu qu·∫£ (ENB)")
        st.markdown("""
        **ENB (Effective Number of Bets)** = 1 / Herfindahl Index  
        ƒêo l∆∞·ªùng s·ªë l∆∞·ª£ng c·ªï phi·∫øu "th·ª±c s·ª± ƒë·ªôc l·∫≠p" trong danh m·ª•c.
        """)
        
        enb_data = {}
        for col in st.session_state.optimal_weights_df.columns:
            weights = st.session_state.optimal_weights_df[col]
            herfindahl = (weights ** 2).sum()
            enb = 1 / herfindahl if herfindahl > 0 else 0
            enb_data[col] = {
                'ENB': enb,
                'S·ªë CP th·ª±c t·∫ø': st.session_state.num_stocks[col],
                'Hi·ªáu qu·∫£ ƒêa d·∫°ng': enb / st.session_state.num_stocks[col] if st.session_state.num_stocks[col] > 0 else 0
            }
        
        enb_df = pd.DataFrame(enb_data).T
        enb_df_styled = enb_df.style.format({
            'ENB': '{:.2f}',
            'S·ªë CP th·ª±c t·∫ø': '{:.0f}',
            'Hi·ªáu qu·∫£ ƒêa d·∫°ng': '{:.1%}'
        }).background_gradient(cmap='RdYlGn', subset=['Hi·ªáu qu·∫£ ƒêa d·∫°ng'])
        
        st.dataframe(enb_df_styled, use_container_width=True)
        
        st.markdown("""
        **Gi·∫£i th√≠ch:**
        - **ENB**: S·ªë c·ªï phi·∫øu c√≥ tr·ªçng s·ªë b·∫±ng nhau t∆∞∆°ng ƒë∆∞∆°ng v·ªõi danh m·ª•c hi·ªán t·∫°i
        - **Hi·ªáu qu·∫£ ƒêa d·∫°ng**: T·ª∑ l·ªá ENB / S·ªë CP th·ª±c t·∫ø (c√†ng cao c√†ng t·ªët, t·ªëi ƒëa 100%)
        - Hi·ªáu qu·∫£ 100% = Ph√¢n b·ªï ho√†n to√†n ƒë·ªÅu, < 50% = T·∫≠p trung cao
        """)
        
        st.divider()
        
        # TH√äM: Risk-Return Scatter c·ªßa t·ª´ng c·ªï phi·∫øu
        st.subheader("Ma tr·∫≠n R·ªßi ro - L·ª£i nhu·∫≠n t·ª´ng C·ªï phi·∫øu")
        
        individual_stats = pd.DataFrame({
            'Return': st.session_state.expected_returns,
            'Risk': np.sqrt(np.diag(st.session_state.cov_matrix))
        })
        individual_stats['Sharpe'] = (individual_stats['Return'] - RISK_FREE_RATE) / individual_stats['Risk']
        
        fig_scatter = px.scatter(
            individual_stats, 
            x='Risk', 
            y='Return',
            text=individual_stats.index,
            color='Sharpe',
            color_continuous_scale='RdYlGn',
            title='Ma tr·∫≠n R·ªßi ro - L·ª£i nhu·∫≠n c·ªßa t·ª´ng C·ªï phi·∫øu trong VN30',
            size=abs(individual_stats['Sharpe']),
            size_max=15
        )
        fig_scatter.update_traces(textposition='top center', textfont_size=8)
        fig_scatter.update_layout(
            template='plotly_dark',
            height=700,
            xaxis_tickformat='.1%',
            yaxis_tickformat='.1%',
            xaxis_title='R·ªßi ro (ƒê·ªô l·ªách chu·∫©n)',
            yaxis_title='L·ª£i nhu·∫≠n K·ª≥ v·ªçng'
        )
        st.plotly_chart(fig_scatter, use_container_width=True)
    
    # Tab 5: Sector Analysis & ML
    with tab5:
        st.header("üè¢ Ph√¢n t√≠ch Ph√¢n b·ªï theo Ng√†nh & Machine Learning")
        
        # === PH·∫¶N 1: SECTOR ANALYSIS ===
        st.subheader("Ph√¢n t√≠ch Ph√¢n b·ªï theo Ng√†nh")
        
        sector_allocation = get_sector_allocation(st.session_state.optimal_weights_df)
        
        # Hi·ªÉn th·ªã b·∫£ng
        st.dataframe(
            sector_allocation.style.format("{:.2%}").background_gradient(cmap='Blues'),
            use_container_width=True
        )
        
        # Bi·ªÉu ƒë·ªì c·ªôt so s√°nh
        sector_long = sector_allocation.reset_index().melt(
            id_vars='index',
            var_name='Danh m·ª•c',
            value_name='T·ª∑ tr·ªçng'
        )
        sector_long.rename(columns={'index': 'Ng√†nh'}, inplace=True)
        
        fig_sector = px.bar(
            sector_long,
            x='Danh m·ª•c',
            y='T·ª∑ tr·ªçng',
            color='Ng√†nh',
            title='Ph√¢n b·ªï theo Ng√†nh - So s√°nh 3 Danh m·ª•c',
            text_auto='.1%'
        )
        fig_sector.update_layout(template='plotly_dark', yaxis_tickformat='.0%', height=500)
        st.plotly_chart(fig_sector, use_container_width=True)
        
        # Bi·ªÉu ƒë·ªì tr√≤n cho t·ª´ng danh m·ª•c
        col1, col2, col3 = st.columns(3)
        
        for idx, (col, portfolio) in enumerate(zip([col1, col2, col3], sector_allocation.columns)):
            with col:
                sector_data = sector_allocation[portfolio]
                sector_data = sector_data[sector_data > 0.001]
                
                fig_sector_pie = go.Figure(data=[go.Pie(
                    labels=sector_data.index,
                    values=sector_data.values,
                    hole=0.3
                )])
                fig_sector_pie.update_layout(
                    title=portfolio,
                    template='plotly_dark',
                    height=400,
                    showlegend=True
                )
                st.plotly_chart(fig_sector_pie, use_container_width=True)
        
        # Ph√¢n t√≠ch ƒëa d·∫°ng h√≥a ng√†nh
        st.subheader("Ch·ªâ s·ªë ƒêa d·∫°ng h√≥a theo Ng√†nh")
        
        sector_diversity = {}
        for portfolio in sector_allocation.columns:
            sectors = sector_allocation[portfolio]
            sectors = sectors[sectors > 0.001]
            hhi = (sectors ** 2).sum()
            enb = 1 / hhi if hhi > 0 else 0
            sector_diversity[portfolio] = {
                'S·ªë ng√†nh': len(sectors),
                'HHI (Ng√†nh)': hhi,
                'ENB (Ng√†nh)': enb,
                'Ng√†nh l·ªõn nh·∫•t': sectors.idxmax(),
                'T·ª∑ tr·ªçng l·ªõn nh·∫•t': sectors.max()
            }
        
        sector_div_df = pd.DataFrame(sector_diversity).T
        st.dataframe(
            sector_div_df.style.format({
                'S·ªë ng√†nh': '{:.0f}',
                'HHI (Ng√†nh)': '{:.3f}',
                'ENB (Ng√†nh)': '{:.2f}',
                'T·ª∑ tr·ªçng l·ªõn nh·∫•t': '{:.2%}'
            }),
            use_container_width=True
        )
        
        st.divider()
        
        # === PH·∫¶N 2: MACHINE LEARNING ===
        st.header("ü§ñ Machine Learning - Ph√¢n t√≠ch N√¢ng cao")
        
        # === PH·∫¶N 1: D·ª∞ ƒêO√ÅN RETURNS ===
        st.subheader("D·ª± ƒëo√°n L·ª£i nhu·∫≠n b·∫±ng Random Forest")
        
        with st.spinner("‚è≥ ƒêang hu·∫•n luy·ªán m√¥ h√¨nh Machine Learning..."):
            predictions, feature_importance = ml_predict_returns(
                st.session_state.returns_df,
                st.session_state.price_pivot
            )
        
        if predictions:
            # Ch·ªçn c·ªï phi·∫øu ƒë·ªÉ xem
            selected_ticker_ml = st.selectbox(
                "Ch·ªçn c·ªï phi·∫øu ƒë·ªÉ xem d·ª± ƒëo√°n:",
                options=list(predictions.keys())
            )
            
            pred_data = predictions[selected_ticker_ml]
            
            # Metrics
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("üéØ ƒê·ªô ch√≠nh x√°c (R¬≤ Score)", f"{pred_data['score']:.3f}")
            with col2:
                mae = np.mean(np.abs(pred_data['actual'] - pred_data['predicted']))
                st.metric("üìä MAE", f"{mae:.4f}")
            with col3:
                rmse = np.sqrt(np.mean((pred_data['actual'] - pred_data['predicted'])**2))
                st.metric("üìâ RMSE", f"{rmse:.4f}")
            
            # Bi·ªÉu ƒë·ªì so s√°nh Actual vs Predicted
            comparison_ml = pd.DataFrame({
                'Th·ª±c t·∫ø': pred_data['actual'],
                'D·ª± ƒëo√°n': pred_data['predicted']
            })
            
            fig_ml = go.Figure()
            fig_ml.add_trace(go.Scatter(
                y=comparison_ml['Th·ª±c t·∫ø'],
                mode='lines',
                name='Th·ª±c t·∫ø',
                line=dict(color='cyan')
            ))
            fig_ml.add_trace(go.Scatter(
                y=comparison_ml['D·ª± ƒëo√°n'],
                mode='lines',
                name='D·ª± ƒëo√°n',
                line=dict(color='orange', dash='dash')
            ))
            fig_ml.update_layout(
                title=f'So s√°nh Returns Th·ª±c t·∫ø vs D·ª± ƒëo√°n - {selected_ticker_ml}',
                template='plotly_dark',
                yaxis_title='Returns',
                xaxis_title='Th·ªùi gian (Test Set)',
                height=500
            )
            st.plotly_chart(fig_ml, use_container_width=True)
            
            # Feature Importance
            st.subheader("ƒê·ªô quan tr·ªçng c·ªßa Features")
            fi_df = feature_importance[selected_ticker_ml].sort_values(ascending=False)
            
            fig_fi = px.bar(
                x=fi_df.values,
                y=fi_df.index,
                orientation='h',
                title=f'Feature Importance - {selected_ticker_ml}',
                labels={'x': 'Importance', 'y': 'Feature'}
            )
            fig_fi.update_layout(template='plotly_dark', height=400)
            st.plotly_chart(fig_fi, use_container_width=True)
            
            st.info("""
            **Gi·∫£i th√≠ch Features:**
            - **return_lag1/2/3**: L·ª£i nhu·∫≠n 1, 2, 3 ng√†y tr∆∞·ªõc
            - **return_ma5/20**: Moving average 5, 20 ng√†y
            - **volatility_20**: ƒê·ªô bi·∫øn ƒë·ªông 20 ng√†y
            """)
        else:
            st.warning("Kh√¥ng ƒë·ªß d·ªØ li·ªáu ƒë·ªÉ hu·∫•n luy·ªán m√¥ h√¨nh ML")
        
        st.divider()
        
        # === PH·∫¶N 2: CLUSTERING ===
        st.subheader("Ph√¢n nh√≥m C·ªï phi·∫øu (K-Means Clustering)")
        
        n_clusters = st.slider(
            "S·ªë nh√≥m (clusters):",
            min_value=2, max_value=5, value=3, step=1
        )
        
        with st.spinner("‚è≥ ƒêang ph√¢n nh√≥m c·ªï phi·∫øu..."):
            cluster_result = cluster_stocks(st.session_state.returns_df, n_clusters)
        
        # Hi·ªÉn th·ªã b·∫£ng
        st.dataframe(
            cluster_result.style.format({
                'mean_return': '{:.4f}',
                'volatility': '{:.4f}',
                'sharpe': '{:.2f}',
                'skewness': '{:.2f}',
                'kurtosis': '{:.2f}'
            }).background_gradient(cmap='viridis', subset=['Cluster']),
            use_container_width=True
        )
        
        # Bi·ªÉu ƒë·ªì scatter 3D
        fig_cluster = px.scatter_3d(
            cluster_result,
            x='mean_return',
            y='volatility',
            z='sharpe',
            color='Cluster',
            text='Ticker',
            title='Ph√¢n nh√≥m C·ªï phi·∫øu theo Risk-Return Profile',
            labels={
                'mean_return': 'Return TB',
                'volatility': 'Volatility',
                'sharpe': 'Sharpe Ratio'
            },
            color_continuous_scale='viridis'
        )
        fig_cluster.update_traces(textposition='top center', textfont_size=8)
        fig_cluster.update_layout(template='plotly_dark', height=700)
        st.plotly_chart(fig_cluster, use_container_width=True)
        
        # Ph√¢n t√≠ch t·ª´ng cluster
        st.subheader("ƒê·∫∑c ƒëi·ªÉm t·ª´ng Nh√≥m")
        
        for cluster_id in range(n_clusters):
            cluster_data = cluster_result[cluster_result['Cluster'] == cluster_id]
            
            with st.expander(f"üîπ Nh√≥m {cluster_id} ({len(cluster_data)} c·ªï phi·∫øu)"):
                col1, col2, col3, col4 = st.columns(4)
                with col1:
                    st.metric("Return TB", f"{cluster_data['mean_return'].mean():.4f}")
                with col2:
                    st.metric("Volatility TB", f"{cluster_data['volatility'].mean():.4f}")
                with col3:
                    st.metric("Sharpe TB", f"{cluster_data['sharpe'].mean():.2f}")
                with col4:
                    st.metric("S·ªë c·ªï phi·∫øu", len(cluster_data))
                
                st.write("**Danh s√°ch c·ªï phi·∫øu:**", ", ".join(cluster_data['Ticker'].tolist()))
        
        st.info("""
        **·ª®ng d·ª•ng Clustering:**
        - X√°c ƒë·ªãnh c√°c c·ªï phi·∫øu c√≥ ƒë·∫∑c t√≠nh t∆∞∆°ng t·ª±
        - ƒêa d·∫°ng h√≥a b·∫±ng c√°ch ch·ªçn c·ªï phi·∫øu t·ª´ c√°c nh√≥m kh√°c nhau
        - Hi·ªÉu r√µ h∆°n v·ªÅ c·∫•u tr√∫c th·ªã tr∆∞·ªùng VN30
        """)
        
        st.divider()
        
        # === PH·∫¶N 3: KHUY·∫æN NGH·ªä ===
        st.subheader("Khuy·∫øn ngh·ªã D·ª±a tr√™n Machine Learning")
        
        # Top c·ªï phi·∫øu theo ML score
        if predictions:
            ml_scores = {ticker: data['score'] for ticker, data in predictions.items()}
            top_ml = sorted(ml_scores.items(), key=lambda x: x[1], reverse=True)[:5]
            
            st.success("**Top 5 c·ªï phi·∫øu d·ª± ƒëo√°n t·ªët nh·∫•t (ML Score cao):**")
            for i, (ticker, score) in enumerate(top_ml, 1):
                st.write(f"{i}. **{ticker}**: R¬≤ Score = {score:.3f}")
        
        # Top c·ªï phi·∫øu theo Sharpe trong m·ªói cluster
        st.warning("**Khuy·∫øn ngh·ªã ƒêa d·∫°ng h√≥a theo Cluster:**")
        for cluster_id in range(n_clusters):
            cluster_data = cluster_result[cluster_result['Cluster'] == cluster_id]
            best_in_cluster = cluster_data.nlargest(1, 'sharpe')
            if not best_in_cluster.empty:
                ticker = best_in_cluster.iloc[0]['Ticker']
                sharpe = best_in_cluster.iloc[0]['sharpe']
                st.write(f"- **Nh√≥m {cluster_id}**: Ch·ªçn **{ticker}** (Sharpe = {sharpe:.2f})")

# [S·ª¨A 2] X√≥a b·ªè d·∫•u } b·ªã l·ªói
# }